{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6308266,"sourceType":"datasetVersion","datasetId":3629391}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"'''!pip uninstall -y numpy\n!pip install --quiet numpy==1.26.4\n\n!pip uninstall -y opencv-python opencv-python-headless\n!pip install --quiet opencv-python-headless==4.8.1.78\n\n!pip uninstall -y albumentations qudida scikit-image scipy scikit-learn imgaug\n!pip install --quiet albumentations==1.3.1'''","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-30T12:02:08.876497Z","iopub.execute_input":"2025-11-30T12:02:08.876749Z","iopub.status.idle":"2025-11-30T12:02:08.887099Z","shell.execute_reply.started":"2025-11-30T12:02:08.876725Z","shell.execute_reply":"2025-11-30T12:02:08.886338Z"}},"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"'!pip uninstall -y numpy\\n!pip install --quiet numpy==1.26.4\\n\\n!pip uninstall -y opencv-python opencv-python-headless\\n!pip install --quiet opencv-python-headless==4.8.1.78\\n\\n!pip uninstall -y albumentations qudida scikit-image scipy scikit-learn imgaug\\n!pip install --quiet albumentations==1.3.1'"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"#import os; os._exit(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T12:02:08.888011Z","iopub.execute_input":"2025-11-30T12:02:08.888393Z","iopub.status.idle":"2025-11-30T12:02:08.898922Z","shell.execute_reply.started":"2025-11-30T12:02:08.888365Z","shell.execute_reply":"2025-11-30T12:02:08.898142Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torchvision\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport numpy as np\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom tqdm import tqdm\nimport torchvision.transforms.functional as TF\nimport cv2\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nimport json\nimport torchvision.models as models\nimport torch.nn.functional as F","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T12:02:08.900917Z","iopub.execute_input":"2025-11-30T12:02:08.901186Z","iopub.status.idle":"2025-11-30T12:02:18.098928Z","shell.execute_reply.started":"2025-11-30T12:02:08.901162Z","shell.execute_reply":"2025-11-30T12:02:18.098141Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"DEVICE = ('cuda' if torch.cuda.is_available else 'cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T12:02:18.099969Z","iopub.execute_input":"2025-11-30T12:02:18.100835Z","iopub.status.idle":"2025-11-30T12:02:18.104461Z","shell.execute_reply.started":"2025-11-30T12:02:18.100809Z","shell.execute_reply":"2025-11-30T12:02:18.103689Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"\nclass RunwayKeypointDataset(Dataset):\n    def __init__(self, img_dir, json_path, transform=None):\n        self.img_dir = img_dir\n        with open(json_path, 'r') as f:\n            self.labels = json.load(f)\n        self.filenames = list(self.labels.keys())\n        \n        self.line_order = ['LEDG', 'REDG', 'CTL']\n        \n        self.width = 640\n        self.height = 360\n\n    def __len__(self):\n        return len(self.filenames)\n\n    def __getitem__(self, idx):\n        # 1. Load Image\n        fname = self.filenames[idx]\n        img_path = os.path.join(self.img_dir, fname)\n        image = cv2.imread(img_path) \n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        raw_points = []\n        validity_mask = []\n        image_data = self.labels[fname]\n        lines_dict = {item['label']: item['points'] for item in image_data}\n        \n        for label in self.line_order:\n            if label in lines_dict:\n                pts = lines_dict[label]\n                raw_points.extend(pts[0]) \n                raw_points.extend(pts[1]) \n                validity_mask.extend([1.0, 1.0, 1.0, 1.0])\n            else:\n                raw_points.extend([0, 0, 0, 0]) \n                validity_mask.extend([0.0, 0.0, 0.0, 0.0])\n\n        coords = np.array(raw_points, dtype=np.float32)\n\n        coords[0::2] /= self.width  \n        coords[1::2] /= self.height \n\n        image = image.transpose(2, 0, 1)\n        image = torch.tensor(image, dtype=torch.float32) / 255.0\n        target = torch.tensor(coords, dtype=torch.float32)\n        mask = torch.tensor(validity_mask, dtype=torch.float32)\n\n        return image, target, mask\n\nclass RunwayDataset(Dataset):\n    def __init__(self, img_dir, mask_dir, transform=None):\n        self.img_dir = img_dir\n        self.mask_dir = mask_dir\n        self.transform = transform\n        self.images = os.listdir(img_dir)\n\n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, index):\n        img_path = os.path.join(self.img_dir, self.images[index])\n        mask_path = os.path.join(self.mask_dir, self.images[index])\n        image = np.array(Image.open(img_path).convert(\"RGB\"))\n        mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.int64)\n\n        if self.transform is not None:\n            aug = self.transform(image=image, mask=mask)\n            image = aug[\"image\"]\n            mask = aug[\"mask\"]\n\n        if isinstance(mask, torch.Tensor):\n            mask = mask.long()\n        else:\n            mask = torch.as_tensor(mask, dtype=torch.long)\n            \n        return image, mask\n\n\ndef masks_from_json(json_path, output_dir, width = 640, height = 360, thickness = 2):\n    os.makedirs(output_dir, exist_ok = True)\n    class_map = {\n        'LEDG' : 1,\n        'REDG' : 2,\n        'CTL' : 3\n    }\n    with open(json_path, 'r') as f:\n        data = json.load(f)\n\n    print(f\"GENERATING MASK IN THE DIRECTORY {output_dir}\")\n    for filename, lines in tqdm(data.items()):\n        mask = np.zeros((height, width), dtype=np.uint8)\n        \n        for line_item in lines:\n            label = line_item['label']\n            \n            if label in class_map:\n                points = line_item['points']\n                pts = np.array(points, dtype=np.int32).reshape((-1, 1, 2))\n                cv2.polylines(mask, [pts], isClosed=False, color=class_map[label], thickness=thickness)\n        \n        save_path = os.path.join(output_dir, filename)\n        cv2.imwrite(save_path, mask)\n\nJSON_TRAIN_PATH = \"/kaggle/input/fs2020-runway-dataset/labels/labels/lines/train_labels_640x360.json\"\nJSON_TEST_PATH = \"/kaggle/input/fs2020-runway-dataset/labels/labels/lines/test_labels_640x360.json\"\n\nTRAIN_MASK_OUTPUT = \"/kaggle/working/masks_train\"\nTEST_MASK_OUTPUT = \"/kaggle/working/masks_test\"\n\n#masks_from_json(JSON_TRAIN_PATH, TRAIN_MASK_OUTPUT)\n#masks_from_json(JSON_TEST_PATH, TEST_MASK_OUTPUT)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T12:02:18.105175Z","iopub.execute_input":"2025-11-30T12:02:18.105469Z","iopub.status.idle":"2025-11-30T12:02:18.130010Z","shell.execute_reply.started":"2025-11-30T12:02:18.105438Z","shell.execute_reply":"2025-11-30T12:02:18.129190Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class RunwayRegressor(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.backbone = models.resnet18(pretrained = True)\n        in_features = self.backbone.fc.in_features\n\n        self.backbone.fc = nn.Sequential(\n            nn.Linear(in_features,256),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(256, 12),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        return self.backbone(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T12:02:18.130874Z","iopub.execute_input":"2025-11-30T12:02:18.131116Z","iopub.status.idle":"2025-11-30T12:02:18.151242Z","shell.execute_reply.started":"2025-11-30T12:02:18.131088Z","shell.execute_reply":"2025-11-30T12:02:18.150460Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import torchvision.transforms.functional as TF\n\nclass DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 3, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, x):\n        return self.conv(x)\n\nclass UNET(nn.Module):\n    def __init__(self, in_channels=3, out_channels=1, features=[64,128,256,512]):\n        super().__init__()\n        self.downs = nn.ModuleList()\n        self.ups = nn.ModuleList()\n        self.pool = nn.MaxPool2d(2)\n\n        for f in features:\n            self.downs.append(DoubleConv(in_channels, f))\n            in_channels = f\n\n        for f in reversed(features):\n            self.ups.append(nn.ConvTranspose2d(f*2, f, kernel_size=2, stride=2))\n            self.ups.append(DoubleConv(f*2, f))\n\n        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n\n    def forward(self, x):\n        skips = []\n        for down in self.downs:\n            x = down(x)\n            skips.append(x)\n            x = self.pool(x)\n\n        x = self.bottleneck(x)\n        skips = skips[::-1]\n\n        for idx in range(0, len(self.ups), 2):\n            x = self.ups[idx](x)\n            skip = skips[idx//2]\n            if x.shape != skip.shape:\n                x = TF.resize(x, skip.shape[2:])\n            x = torch.cat((skip, x), dim=1)\n            x = self.ups[idx+1](x)\n\n        return self.final_conv(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T12:02:18.152726Z","iopub.execute_input":"2025-11-30T12:02:18.153030Z","iopub.status.idle":"2025-11-30T12:02:18.165552Z","shell.execute_reply.started":"2025-11-30T12:02:18.153012Z","shell.execute_reply":"2025-11-30T12:02:18.165018Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def save_visual_predictions(loader, model, folder, device):\n    os.makedirs(folder, exist_ok=True)\n    model.eval()\n    \n    x, y, valid_mask = next(iter(loader))\n    x = x.to(device)\n    y = y.to(device)\n    valid_mask = valid_mask.to(device)\n    \n    with torch.no_grad():\n        preds = model(x).cpu().numpy()\n        y = y.cpu().numpy()\n        valid_mask = valid_mask.cpu().numpy()\n    \n    x = x.cpu().numpy().transpose(0, 2, 3, 1)\n    x = (x * 255).astype(np.uint8)\n    \n    def draw_lines_on_image(coords, background_img, validity=None):\n        img = background_img.copy()\n        height, width = img.shape[:2] \n        \n        c = coords.copy()\n        c[0::2] *= width \n        c[1::2] *= height \n        c = c.astype(int)\n        \n        colors = [(255, 0, 0), (0, 0, 255), (0, 255, 0)]\n        \n        for i in range(3):\n            start_idx = i * 4\n            \n            if validity is not None and validity[start_idx] == 0:\n                continue\n                \n            pt1 = (c[start_idx], c[start_idx+1])\n            pt2 = (c[start_idx+2], c[start_idx+3])\n            \n            cv2.line(img, pt1, pt2, colors[i], 3)\n            \n        return img\n\n    num_to_save = min(5, x.shape[0])\n    \n    for i in range(num_to_save):\n        original_img = x[i].copy()\n        \n        gt_overlay = draw_lines_on_image(y[i], original_img, validity=valid_mask[i])\n        \n        pred_overlay = draw_lines_on_image(preds[i], original_img)\n        \n        combined = np.hstack((original_img, gt_overlay, pred_overlay))\n        \n        # E. Save\n        combined_bgr = cv2.cvtColor(combined, cv2.COLOR_RGB2BGR)\n        cv2.imwrite(f\"{folder}/val_pred_{i}.png\", combined_bgr)\n        \n    model.train()\n\n\ndef save_multiclass_predictions(loader, model, folder=\"saved_images/\", device=\"cuda\"):\n    os.makedirs(folder, exist_ok=True)\n    model.eval()\n    \n    x, y = next(iter(loader))\n    x = x.to(device)\n    y = y.to(device) \n\n    with torch.no_grad():\n        preds = model(x)          \n        pred_labels = torch.argmax(preds, dim=1) \n\n    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1).to(device)\n    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1).to(device)\n\n    num_to_save = min(5, x.shape[0])\n    print(f\"Saving {num_to_save} comparison images to {folder}...\")\n\n    for i in range(num_to_save):\n        img_tensor = x[i] * std + mean\n        img_vis = img_tensor.cpu().numpy().transpose(1, 2, 0)\n        img_vis = (img_vis * 255).clip(0, 255).astype(np.uint8)\n\n        gt_mask_rgb = visualize_multiclass_prediction(y[i])\n        \n        pred_mask_rgb = visualize_multiclass_prediction(pred_labels[i])\n        \n        combined = np.hstack((img_vis, gt_mask_rgb, pred_mask_rgb))\n        \n        combined_bgr = cv2.cvtColor(combined, cv2.COLOR_RGB2BGR)\n        cv2.imwrite(f\"{folder}/comparison_{i}.png\", combined_bgr)\n        \n    model.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T12:02:18.167260Z","iopub.execute_input":"2025-11-30T12:02:18.167485Z","iopub.status.idle":"2025-11-30T12:02:18.186366Z","shell.execute_reply.started":"2025-11-30T12:02:18.167468Z","shell.execute_reply":"2025-11-30T12:02:18.185552Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class DiceLoss(nn.Module):\n    def __init__(self, smooth=1e-8):\n        super().__init__()\n        self.smooth = smooth\n\n    def forward(self, preds, targets):\n        preds = torch.sigmoid(preds)\n        intersection = (preds * targets).sum()\n        dice = 1 - (2 * intersection + self.smooth) / (preds.sum() + targets.sum() + self.smooth)\n        return dice.mean()\n\n\nclass MultiClassTverskyLoss(nn.Module):\n    def __init__(self, alpha=0.7, beta=0.3, smooth=1.0):\n        super(MultiClassTverskyLoss, self).__init__()\n        self.alpha = alpha\n        self.beta = beta    \n        self.smooth = smooth\n\n    def forward(self, inputs, targets):\n        \n        probs = F.softmax(inputs, dim=1)\n        \n        targets_one_hot = F.one_hot(targets, num_classes=inputs.shape[1]).permute(0, 3, 1, 2).float()\n        \n        tp = (probs * targets_one_hot).sum(dim=(0, 2, 3))\n        fp = (probs * (1 - targets_one_hot)).sum(dim=(0, 2, 3))\n        fn = ((1 - probs) * targets_one_hot).sum(dim=(0, 2, 3))\n        \n        tversky = (tp + self.smooth) / (tp + self.alpha * fp + self.beta * fn + self.smooth)\n        \n        return 1 - tversky.mean()\n\n\nclass_weights = torch.tensor([1.0, 50.0, 50.0, 50.0]).to(DEVICE)\nce_loss_fn = nn.CrossEntropyLoss(weight=class_weights)\ntversky_loss_fn = MultiClassTverskyLoss(alpha=0.7, beta=0.3)\n\ndef loss_fn(preds, targets):\n    ce = ce_loss_fn(preds, targets)\n    \n    tversky = tversky_loss_fn(preds, targets)\n    \n    return 0.5 * ce + 0.5 * tversky","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T12:02:18.187840Z","iopub.execute_input":"2025-11-30T12:02:18.188063Z","iopub.status.idle":"2025-11-30T12:02:18.471921Z","shell.execute_reply.started":"2025-11-30T12:02:18.188038Z","shell.execute_reply":"2025-11-30T12:02:18.471028Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def check_accuracy(loader, model, device=\"cuda\"):\n    dice = 0\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device)\n            y = y.to(device).unsqueeze(1)\n            raw_scores = model(x)\n            probs = torch.softmax(raw_scores, dim = 1)\n            preds = torch.argmax(probs, dim = 1)\n            intersection = (preds * y).sum()\n            dice += (2*intersection + 1e-8) / (preds.sum() + y.sum() + 1e-8)\n    print(\"DICE SCORE:\", dice/len(loader))\n    model.train()\n\ndef save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n    print(\"SAVING CHECK POINT......\")\n    torch.save(state, filename)\n\ndef load_checkpoint(checkpoint, model):\n    print(\"......LOADING CHECKPOINT\")\n    model.load_state_dict(checkpoint[\"state_dict\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T12:02:18.472761Z","iopub.execute_input":"2025-11-30T12:02:18.473021Z","iopub.status.idle":"2025-11-30T12:02:18.479233Z","shell.execute_reply.started":"2025-11-30T12:02:18.472998Z","shell.execute_reply":"2025-11-30T12:02:18.478373Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"TRAIN_IMG_DIR = r\"/kaggle/input/fs2020-runway-dataset/640x360/640x360/train\"\nTEST_IMG_DIR = r\"/kaggle/input/fs2020-runway-dataset/640x360/640x360/test\"\nTRAIN_MASK_OUTPUT = \"/kaggle/working/masks_train\"\nTEST_MASK_OUTPUT = \"/kaggle/working/masks_test\"\nJSON_TRAIN_PATH = r\"/kaggle/input/fs2020-runway-dataset/labels/labels/lines/train_labels_640x360.json\"\nJSON_TEST_PATH = r\"/kaggle/input/fs2020-runway-dataset/labels/labels/lines/test_labels_640x360.json\"\n\nBATCH_SIZE = 16\nLR = 1e-4\nEPOCHS = 25\nIMG_H = 360\nIMG_W = 640\n\ntrain_tf = A.Compose([\n    A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),\n    A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n    A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n    A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.3),\n])\n\ntest_tf = A.Compose([\n    A.Resize(IMG_H, IMG_W),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0),\n    ToTensorV2(),\n])\n\n\ntrain_ds = RunwayKeypointDataset(TRAIN_IMG_DIR, JSON_TRAIN_PATH, train_tf)\ntest_ds = RunwayKeypointDataset(TEST_IMG_DIR, JSON_TEST_PATH)\n\n\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\ntest_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, num_workers=4, pin_memory=True)\n\n\nmodel = RunwayRegressor().to(DEVICE)\noptimizer = torch.optim.Adam(model.parameters(), lr=LR)\n\nscaler = torch.amp.GradScaler('cuda')\n\nprint(f\"Starting Training with Batch Size {BATCH_SIZE}...\")\nbest_loss = float('inf')\n\nfor epoch in range(EPOCHS):\n    model.train()\n    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n    train_loss = 0\n    \n    for x, y, mask in loop:\n        x, y, mask = x.to(DEVICE), y.to(DEVICE), mask.to(DEVICE)\n        \n        preds = model(x)\n        \n        raw_error = torch.abs(preds - y)\n        masked_error = raw_error * mask\n        loss = masked_error.sum() / (mask.sum() + 1e-6)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        train_loss += loss.item()\n        loop.set_postfix(loss=loss.item())\n        \n    model.eval()\n    val_loss = 0\n    with torch.no_grad():\n        for x, y, mask in test_loader:\n            x, y, mask = x.to(DEVICE), y.to(DEVICE), mask.to(DEVICE)\n            preds = model(x)\n            \n            raw_error = torch.abs(preds - y)\n            masked_error = raw_error * mask\n            loss = masked_error.sum() / (mask.sum() + 1e-6)\n            \n            val_loss += loss.item()\n            \n    val_loss /= len(test_loader)\n    print(f\"Epoch {epoch+1} | Train Loss: {train_loss/len(train_loader):.5f} | Val Loss: {val_loss:.5f}\")\n\n    if val_loss < best_loss:\n        best_loss = val_loss\n        torch.save(model.state_dict(), \"best_regressor.pth\")\n        \n    if (epoch+1) % 5 == 0:\n        save_visual_predictions(test_loader, model, f\"output_images/epoch_{epoch+1}\", DEVICE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T12:02:18.480148Z","iopub.execute_input":"2025-11-30T12:02:18.480439Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_47/2232136034.py:17: UserWarning: Argument(s) 'var_limit' are not valid for transform GaussNoise\n  A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00<00:00, 177MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Starting Training with Batch Size 16...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/25: 100%|██████████| 249/249 [00:50<00:00,  4.97it/s, loss=0.0873]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 | Train Loss: 0.08213 | Val Loss: 0.06256\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/25: 100%|██████████| 249/249 [00:53<00:00,  4.66it/s, loss=0.0527]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 | Train Loss: 0.06714 | Val Loss: 0.05757\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/25: 100%|██████████| 249/249 [00:52<00:00,  4.73it/s, loss=0.089] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 | Train Loss: 0.06095 | Val Loss: 0.05170\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/25: 100%|██████████| 249/249 [00:52<00:00,  4.71it/s, loss=0.0507]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 | Train Loss: 0.05488 | Val Loss: 0.04919\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/25: 100%|██████████| 249/249 [00:53<00:00,  4.69it/s, loss=0.0478]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 | Train Loss: 0.04823 | Val Loss: 0.04290\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/25: 100%|██████████| 249/249 [00:52<00:00,  4.71it/s, loss=0.0432]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6 | Train Loss: 0.04285 | Val Loss: 0.03643\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/25: 100%|██████████| 249/249 [00:52<00:00,  4.72it/s, loss=0.0265]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7 | Train Loss: 0.03870 | Val Loss: 0.03180\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/25: 100%|██████████| 249/249 [00:52<00:00,  4.73it/s, loss=0.0322]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8 | Train Loss: 0.03554 | Val Loss: 0.02866\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/25: 100%|██████████| 249/249 [00:52<00:00,  4.72it/s, loss=0.0308]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9 | Train Loss: 0.03362 | Val Loss: 0.03096\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/25: 100%|██████████| 249/249 [00:52<00:00,  4.71it/s, loss=0.0472]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10 | Train Loss: 0.03163 | Val Loss: 0.02862\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/25: 100%|██████████| 249/249 [00:52<00:00,  4.72it/s, loss=0.0364]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11 | Train Loss: 0.03018 | Val Loss: 0.02647\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/25: 100%|██████████| 249/249 [00:52<00:00,  4.74it/s, loss=0.0384]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12 | Train Loss: 0.02893 | Val Loss: 0.02505\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/25:  57%|█████▋    | 143/249 [00:30<00:22,  4.74it/s, loss=0.0239]","output_type":"stream"}],"execution_count":null}]}